{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'$': 0, '0': 1, '8': 2, '5': 3, '7': 4, '4': 5, '9': 6, ' ': 7, '*': 8, '1': 9, '6': 10, '=': 11, '2': 12, '3': 13}\n",
      "Got : tok_dict {'$': 0, '0': 1, '8': 2, '5': 3, '7': 4, '4': 5, '9': 6, ' ': 7, '*': 8, '1': 9, '6': 10, '=': 11, '2': 12, '3': 13}\n",
      "Got : tok_dict_rev {0: '$', 1: '0', 2: '8', 3: '5', 4: '7', 5: '4', 6: '9', 7: ' ', 8: '*', 9: '1', 10: '6', 11: '=', 12: '2', 13: '3'}\n",
      "Dataset contains 3000.00M tokens, resulting in 100000k examples.\n",
      "phrase : tensor([ 5,  5,  3, 11, 12])\n",
      "Input :  $2=54\n",
      "Truth :  2=544\n",
      "tensor([ 0, 12, 11,  3,  5])\n",
      "tensor([12, 11,  3,  5,  5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from modules import SimpleTokenizer\n",
    "from modules import TokenTextBOS, TokenTextBOSAttention\n",
    "import random\n",
    "\n",
    "tok_dict_loc = 'tokenizers/primes.pt'\n",
    "print(torch.load(tok_dict_loc))\n",
    "tokenizer = SimpleTokenizer(tok_dict_loc=tok_dict_loc)\n",
    "\n",
    "dataset = TokenTextBOSAttention('h5data/primes1_5.h5', backwards=True,attention_size=5)\n",
    "\n",
    "i = random.randint(0,len(dataset))\n",
    "example = dataset[i]\n",
    "print('Input : ', tokenizer.detokenize(example[0]))\n",
    "print('Truth : ', tokenizer.detokenize(example[1]))\n",
    "print('Full : ', tokenizer.detokenize(example[2]))\n",
    "print(example[0])\n",
    "print(example[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'$': 0, '4': 1, '3': 2, '1': 3, ' ': 4, '*': 5, '8': 6, '9': 7, '0': 8, '5': 9, '7': 10, '=': 11, '2': 12, '6': 13}\n",
      "Got : tok_dict {'$': 0, '4': 1, '3': 2, '1': 3, ' ': 4, '*': 5, '8': 6, '9': 7, '0': 8, '5': 9, '7': 10, '=': 11, '2': 12, '6': 13}\n",
      "Got : tok_dict_rev {0: '$', 1: '4', 2: '3', 3: '1', 4: ' ', 5: '*', 6: '8', 7: '9', 8: '0', 9: '5', 10: '7', 11: '=', 12: '2', 13: '6'}\n",
      "Dataset contains 0.03M tokens, resulting in 1k examples.\n",
      "Dataset contains 0.03M tokens, resulting in 1k examples.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "range object index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 32>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m train_forward \u001b[38;5;241m=\u001b[39m Subset(motherForward, keep_range)\n\u001b[0;32m     30\u001b[0m val_forward \u001b[38;5;241m=\u001b[39m Subset(motherForward, val_range)\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput back: \u001b[39m\u001b[38;5;124m'\u001b[39m, tokenizer\u001b[38;5;241m.\u001b[39mdetokenize(\u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput forw: \u001b[39m\u001b[38;5;124m'\u001b[39m, tokenizer\u001b[38;5;241m.\u001b[39mdetokenize(train_forward[\u001b[38;5;241m7\u001b[39m][\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTruth back:\u001b[39m\u001b[38;5;124m'\u001b[39m, tokenizer\u001b[38;5;241m.\u001b[39mdetokenize(train_dataset[\u001b[38;5;241m7\u001b[39m][\u001b[38;5;241m1\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\vassi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[1;32m--> 295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m]\n",
      "\u001b[1;31mIndexError\u001b[0m: range object index out of range"
     ]
    }
   ],
   "source": [
    "# ---------------- GALOIS ----------------\n",
    "import torch\n",
    "from modules import SimpleTokenizer\n",
    "from modules import TokenTextBOS\n",
    "import pickle\n",
    "from torch.utils.data import Subset\n",
    "import random\n",
    "\n",
    "tok_dict_loc = 'test_toki/prime-products_1_5.pt'\n",
    "print(torch.load(tok_dict_loc))\n",
    "tokenizer = SimpleTokenizer(tok_dict_loc=tok_dict_loc)\n",
    "\n",
    "motherDataset = TokenTextBOS(h5_file='h5data/last_tokenization.h5', backwards=True)\n",
    "motherForward = TokenTextBOS(h5_file='h5data/last_tokenization.h5', backwards=False)\n",
    "\n",
    "indices = list(range(len(motherDataset)))\n",
    "random.shuffle(indices)\n",
    "motherDataset = Subset(motherDataset, indices) # Shuffled dataset\n",
    "\n",
    "# To keep it constant even if switching batch_size, I take batch_size=250\n",
    "val_inds = 2\n",
    "val_range = range(len(motherDataset)-val_inds,len(motherDataset)) # Validation, last portion of dataset\n",
    "keep_range = range(len(motherDataset)-val_inds) # Training, first portion of dataset\n",
    "\n",
    "#Whether backwards or forwards, its the individual examples that are flipped, not the dataset. So same thing for both !\n",
    "train_dataset = Subset(motherDataset, keep_range)\n",
    "val_dataset = Subset(motherDataset, val_range)\n",
    "\n",
    "train_forward = Subset(motherForward, keep_range)\n",
    "val_forward = Subset(motherForward, val_range)\n",
    "\n",
    "print('Input back: ', tokenizer.detokenize(train_dataset[7][0]))\n",
    "print('Input forw: ', tokenizer.detokenize(train_forward[7][0]))\n",
    "\n",
    "print('Truth back:', tokenizer.detokenize(train_dataset[7][1]))\n",
    "print('Truth forw:', tokenizer.detokenize(train_forward[7][1]))\n",
    "\n",
    "\n",
    "print('Input back: ', tokenizer.detokenize(val_dataset[7][0]))\n",
    "print('Input forw: ', tokenizer.detokenize(val_forward[7][0]))\n",
    "\n",
    "print('Truth back:', tokenizer.detokenize(val_dataset[7][1]))\n",
    "print('Truth forw:', tokenizer.detokenize(val_forward[7][1]))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
